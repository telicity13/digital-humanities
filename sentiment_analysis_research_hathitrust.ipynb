{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Sentiment Analysis Research with the HathiTrust Digital Library"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Contents\n",
    "1. Overview\n",
    "2. Acquiring the Textual Data\n",
    "3. Installing Dependencies\n",
    "4. Fiction Example: _The Count of Monte Cristo_\n",
    "   1. Full Text Analysis\n",
    "   2. Extracted Features Analysis\n",
    "   3. Emotional Valence Graph\n",
    "5. Nonfiction Example: _The Origin of Species_\n",
    "   1. Full Text Analysis\n",
    "   2. Extracted Features Analysis\n",
    "   3. Emotional Valence Graph\n",
    "6. Exploring Large Language Models (LLMs)\n",
    "7. Conclusion\n",
    "8. Further Readings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "In this project, I conducted sentiment analysis using Python on two volumes from the HathiTrust Digital Library to examine how the emotional valence changes across each text. The book titles I analyzed were one fiction novel _[The Count of Monte Cristo](https://hdl.handle.net/2027/mdp.39015062136661)_ by Alexandre Dumas and one nonfiction text _[The Origin of Species](https://hdl.handle.net/2027/hvd.hw39sc)_ by Charles Darwin. My goal was to generate visualizations of the change in emotional valence over the span of these books.\n",
    "\n",
    "I utilized two forms of textual data for my analysis: full text (TXT) files downloaded directly from HathiTrust Digital Library, as well as Extracted Features (EF) obtained from [HathiTrust Research Center (HTRC) Analytics](https://analytics.hathitrust.org/). HTRC Analytics enables non-profit research and educational uses of materials in the HathiTrust collection, including those still under copyright. Specifically, the [Extracted Features](https://analytics.hathitrust.org/datasets) contain metadata about volumes and pages alongside part-of-speech-tagged tokens and token counts extracted from full texts.\n",
    "\n",
    "With this textual data, I performed sentiment analysis using three tools: VADER, TextBlob, and AFINN. Each tool assigns sentiment scores to input texts which can be aggregated and visualized to show how emotional valence shifts across the span of pages in each book.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Acquiring the Textual Data\n",
    "To download the full-text files for entire volumes from HathiTrust, one needs to be affiliated with a HathiTrust [member institution](https://www.hathitrust.org/member-libraries/member-list/) and logged into the HathiTrust website using institutional credentials.\n",
    "\n",
    "Full text files can be directly downloaded from each item's page. Extracted features must be accessed through [HTRC Analytics](https://analytics.hathitrust.org/), following this [EF download tutorial](https://htrc.atlassian.net/wiki/spaces/COM/pages/43288147/Downloading+Extracted+Features#DownloadingExtractedFeatures-EF1.5download).\n",
    "\n",
    "For this project, I downloaded the following data files:\n",
    "\n",
    "_The Count of Monte Cristo_: `mdp-39015062136661-1693964099.txt` (full text) and `mdp.39015062136661.json.bz2` (Extracted Features)\n",
    "\n",
    "_On the Origin of Species_: `hvd-hw39sc-1696432701.txt` (full text) and `hvd.hw39sc.json.bz2` (Extracted Features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Installing Dependencies\n",
    "In this project, I have selected three widely-used sentiment analysis tools:\n",
    "\n",
    "1. **[VADER](https://www.nltk.org/index.html)**: Part of the NLTK (Natural Language Toolkit), VADER is specifically designed for social media texts, adept at handling informal language, emojis, and slang.\n",
    "2. **[TextBlob](https://textblob.readthedocs.io/en/dev/)**: A user-friendly library, TextBlob simplifies many common natural language processing (NLP) tasks, including sentiment analysis.\n",
    "3. **[AFINN](https://pypi.org/project/afinn/)**: AFINN is a wordlist-based tool for sentiment analysis where each word in the list is rated for its sentiment strength.\n",
    "\n",
    "These tools can be installed in a Python environment using `pip` commands:\n",
    "```\n",
    "pip install nltk\n",
    "pip install textblob\n",
    "pip install afinn\n",
    "```\n",
    "\n",
    "To analyze Extracted Features, **[htrc-feature-reader](https://pypi.org/project/htrc-feature-reader/)** is needed. It is a tool designed specifically to work with Extracted Features from HTRC.\n",
    "```\n",
    "pip install htrc-feature-reader\n",
    "```\n",
    "\n",
    "Other modules needed for the project include `pandas` and `plotly`. The first tool is used for data analysis, and the second is for plotting interactive graphs.\n",
    "```\n",
    "pip install pandas\n",
    "pip install plotly==5.18.0\n",
    "```\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fiction Example: _The Count of Monte Cristo_\n",
    "#### Full Text Analysis\n",
    "\n",
    "With the texts downloaded and the required dependencies installed, I was ready to begin the sentiment analysis process. The first step was importing the necessary libraries and modules:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import libraries for data analysis and visualization\n",
    "import re  # for regular expression operations\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Import the sentiment analysis tools\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from afinn import Afinn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The full-text file contained the entire book of _The Count of Monte Cristo_ without any page breaks. To analyze the text, I needed to structure it into a more manageable format. I aimed to turn the text into a DataFrame with two columns: one for page numbers and the other for the content on each page.\n",
    "\n",
    "While looking at the TXT file, I noticed markers that indicate page breaks, formatted as `## p. (#1) #################################################`. I decided to use regular expressions (RegEx) to identify these markers and parse the text accordingly.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Open the file and split the text into lines\n",
    "with open('mdp-39015062136661-1693964099.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "lines = text.split('\\n')\n",
    "\n",
    "# Initialize lists for page numbers and content\n",
    "page_numbers = []\n",
    "page_content = []\n",
    "\n",
    "current_page_number = None\n",
    "current_page_content = []\n",
    "\n",
    "# Parse the text with RegEx\n",
    "for line in lines:\n",
    "    if line.startswith(\"## p. \"):\n",
    "        if current_page_number is not None:\n",
    "            page_numbers.append(current_page_number)\n",
    "            page_content.append(\" \".join(current_page_content))\n",
    "\n",
    "        page_pattern = r'## p\\. (\\d+)'\n",
    "        match = re.match(page_pattern, line)\n",
    "        if match:\n",
    "            current_page_number = match.group(1)\n",
    "            current_page_content = []\n",
    "    else:\n",
    "        current_page_content.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I created a DataFrame named `dumas_full_text` to organize `page_numbers` and `page_content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "dumas_full_text = pd.DataFrame({\n",
    "    'page_number': [int(x) for x in page_numbers],\n",
    "    'page_content': page_content,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is a preview of the DataFrame for pages 11 to 15:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "dumas_full_text[10:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VADER\n",
    "\n",
    "With the DataFrame ready, I proceeded with sentiment analysis using the VADER tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "dumas_full_text['vader_sentiment_score'] = 0.0\n",
    "dumas_full_text['vader_sentiment'] = \"\"\n",
    "\n",
    "for tuple in dumas_full_text.itertuples():\n",
    "    sentence = tuple.page_content\n",
    "\n",
    "    sentiment_dictionary = analyzer.polarity_scores(sentence)\n",
    "    compound = sentiment_dictionary['compound']\n",
    "\n",
    "    dumas_full_text.at[tuple.Index, 'vader_sentiment_score'] = compound\n",
    "\n",
    "    if compound >= 0.33:\n",
    "        vader_sentiment = \"Positive\"\n",
    "    elif compound <= -0.33:\n",
    "        vader_sentiment = \"Negative\"\n",
    "    else:\n",
    "        vader_sentiment = \"Neutral\"\n",
    "\n",
    "    dumas_full_text.at[tuple.Index, 'vader_sentiment'] = vader_sentiment\n",
    "\n",
    "dumas_full_text[10:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TextBlob\n",
    "\n",
    "The process for sentiment analysis with TextBlob is similar to that with VADER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "dumas_full_text['textblob_sentiment_score'] = 0.0\n",
    "dumas_full_text['textblob_sentiment'] = \"\"\n",
    "\n",
    "for tuple in dumas_full_text.itertuples():\n",
    "    sentence = tuple.page_content\n",
    "\n",
    "    classifier = TextBlob(sentence)\n",
    "    polarity = classifier.sentiment.polarity\n",
    "\n",
    "    dumas_full_text.at[tuple.Index, 'textblob_sentiment_score'] = polarity\n",
    "\n",
    "    if polarity >= 0.1:\n",
    "        textblob_sentiment = \"Positive\"\n",
    "    elif polarity <= -0.1:\n",
    "        textblob_sentiment = \"Negative\"\n",
    "    else:\n",
    "        textblob_sentiment = \"Neutral\"\n",
    "\n",
    "    dumas_full_text.at[tuple.Index, 'textblob_sentiment'] = textblob_sentiment\n",
    "\n",
    "dumas_full_text[10:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AFINN\n",
    "\n",
    "Lastly, I used AFINN to analyze the sentiment across the full text of _The Count of Monte Cristo_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "afinn = Afinn(language='en')\n",
    "\n",
    "dumas_full_text['afinn_sentiment_score'] = 0.0\n",
    "\n",
    "for tuple in dumas_full_text.itertuples():\n",
    "    sentence = tuple.page_content\n",
    "\n",
    "    score = afinn.score(sentence)\n",
    "\n",
    "    dumas_full_text.at[tuple.Index, 'afinn_sentiment_score'] = score\n",
    "\n",
    "dumas_full_text[10:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "One thing to note is that the scale of the `afinn_sentiment_score` is different from the scales of `vader_sentiment_score` and `textblob_sentiment_score`. While VADER and TextBlob's range is between -1 and 1, AFINN's scores are sums of sentiment values of individual words, which resulted in much larger absolute values. Therefore, I needed to normalize AFINN to the range of -1 to 1."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Normalize the AFINN sentiment scores\n",
    "min_value = min(dumas_full_text['afinn_sentiment_score'])\n",
    "max_value = max(dumas_full_text['afinn_sentiment_score'])\n",
    "\n",
    "normalized_numbers = [(x - min_value) / (max_value - min_value) for x in dumas_full_text['afinn_sentiment_score']]\n",
    "\n",
    "# Adjust the normalized numbers to the -1 to 1 range\n",
    "afinn_normalized = [2 * x - 1 for x in normalized_numbers]\n",
    "\n",
    "dumas_full_text['afinn_normalized'] = afinn_normalized\n",
    "\n",
    "dumas_full_text[10:15]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Extracted Features Analysis\n",
    "Moving on to analyzing the Extracted Features, I first imported `FeatureReader` from the `htrc_features` library."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from htrc_features import FeatureReader\n",
    "import warnings\n",
    "# The warnings are suppressed to avoid clutter in the output and do not affect the program's functionality\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "paths = ['mdp.39015062136661.json.bz2']\n",
    "fr = FeatureReader(paths)\n",
    "vol = next(fr.volumes())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, I created a DataFrame for Extracted Features named `dumas_ef`. I also grouped the tokens by page number, so that I could analyze the content on each page in a way similar to full text."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dumas_ef = vol.tokenlist(pos=False, case=False)\\\n",
    "        .reset_index().drop(['section'], axis=1)\n",
    "dumas_ef.columns = ['Page Number', 'token', 'count']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Group tokens by page number\n",
    "grouped_tokens = dumas_ef.groupby('Page Number')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next section of the code combines the sentiment analysis of the EF with the three tools: VADER, TextBlob and AFINN."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize lists to store sentiment analysis results\n",
    "ef_vader_sentiment_score = []\n",
    "ef_textblob_sentiment_score = []\n",
    "ef_afinn_sentiment_score = []\n",
    "\n",
    "# Perform sentiment analysis for each page\n",
    "for name, group in grouped_tokens:\n",
    "    page_text = \" \".join([word * count for word, count in zip(group['token'], group['count'])])\n",
    "\n",
    "    # VADER Analysis\n",
    "    sentiment_scores = analyzer.polarity_scores(page_text)\n",
    "    ef_vader_sentiment_score.append(sentiment_scores['compound'])\n",
    "\n",
    "    # TextBlob Analysis\n",
    "    sentiment = TextBlob(page_text).sentiment\n",
    "    ef_textblob_sentiment_score.append(sentiment.polarity)\n",
    "\n",
    "    # AFINN Analysis\n",
    "    sentiment_score = afinn.score(page_text)\n",
    "    ef_afinn_sentiment_score.append(sentiment_score)\n",
    "\n",
    "# Create a DataFrame with all sentiment analysis results\n",
    "dumas_ef = pd.DataFrame({\n",
    "    'page_number': [int(x) for x in grouped_tokens.groups.keys()],\n",
    "    'ef_vader_sentiment_score': ef_vader_sentiment_score,\n",
    "    'ef_textblob_sentiment_score': ef_textblob_sentiment_score,\n",
    "    'ef_afinn_sentiment_score': ef_afinn_sentiment_score\n",
    "})\n",
    "\n",
    "dumas_ef[10:15]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Again, I needed to normalize the AFINN scores."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "min_value = min(dumas_ef['ef_afinn_sentiment_score'])\n",
    "max_value = max(dumas_ef['ef_afinn_sentiment_score'])\n",
    "\n",
    "normalized_numbers = [(x - min_value) / (max_value - min_value) for x in dumas_ef['ef_afinn_sentiment_score']]\n",
    "ef_afinn_normalized = [2 * x - 1 for x in normalized_numbers]\n",
    "\n",
    "dumas_ef['ef_afinn_normalized'] = ef_afinn_normalized\n",
    "\n",
    "dumas_ef[['ef_afinn_sentiment_score', 'ef_afinn_normalized']][10:15]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Another thing that needed to be adjusted is the page numbers in the `dumas_ef` DataFrame. The original page numbers in `dumas_ef` were higher because they included pages from the book's front matter, such as the cover and copyright pages. By subtracting 16, the number of front matter of this book, I ensured that the page numbers in `dumas_ef` match those in `dumas_full_text` for meaningful comparison."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "FRONT_MATTER_PAGES = 16\n",
    "dumas_ef['page_number'] = dumas_ef['page_number'].astype(int) - FRONT_MATTER_PAGES"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dumas_ef[10:15]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Emotional Valence Graph\n",
    "\n",
    "My goal is to visualize the overall emotional trend throughout the book. For this, a detailed granularity is not necessary. Therefore, I used a rolling mean with a window size of 20 pages to smooth out the data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 20\n",
    "\n",
    "# Columns in dumas_full_text for rolling window operation\n",
    "columns_full_text = ['vader_sentiment_score', 'textblob_sentiment_score', 'afinn_normalized']\n",
    "\n",
    "for col in columns_full_text:\n",
    "    dumas_full_text[col] = dumas_full_text[col].rolling(window=WINDOW_SIZE, min_periods=1).mean()\n",
    "\n",
    "# Columns in dumas_ef for rolling window operation\n",
    "columns_ef = ['ef_vader_sentiment_score', 'ef_textblob_sentiment_score', 'ef_afinn_normalized']\n",
    "\n",
    "for col in columns_ef:\n",
    "    dumas_ef[col] = dumas_ef[col].rolling(window=WINDOW_SIZE, min_periods=1).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, I used `plotly` to create an interactive graph with all the processed data. The interactive graph enables the viewer to toggle which graph(s) they would like to see and allows for a more straightforward comparison."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "# Plotting for dumas_full_text DataFrame\n",
    "sentiment_scores_full_text = {\n",
    "    'vader_sentiment_score': 'Full Text VADER',\n",
    "    'textblob_sentiment_score': 'Full Text TextBlob',\n",
    "    'afinn_normalized': 'Full Text AFINN Normalized'\n",
    "}\n",
    "\n",
    "for column, label in sentiment_scores_full_text.items():\n",
    "    fig.add_trace(go.Scatter(x=dumas_full_text['page_number'], y=dumas_full_text[column], mode='lines', name=label))\n",
    "\n",
    "# Plotting for dumas_ef DataFrame\n",
    "sentiment_scores_ef = {\n",
    "    'ef_vader_sentiment_score': 'EF VADER',\n",
    "    'ef_textblob_sentiment_score': 'EF TextBlob',\n",
    "    'ef_afinn_normalized': 'EF AFINN Normalized'\n",
    "}\n",
    "\n",
    "for column, label in sentiment_scores_ef.items():\n",
    "    fig.add_trace(go.Scatter(x=dumas_ef['page_number'], y=dumas_ef[column], mode='lines', name=label))\n",
    "\n",
    "# Additional plot settings\n",
    "fig.update_layout(\n",
    "    title=f'Emotional Valence throughout \"The Count of Monte Cristo\"',\n",
    "    xaxis_title='Page Number',\n",
    "    yaxis_title='Sentiment Score',\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Nonfiction Example: _The Origin of Species_\n",
    "#### Full Text Analysis\n",
    "Moving on to the nonfiction example: _The Origin of Species_. The process was largely the same, with only a few minor differences.\n",
    "\n",
    "In this book's full-text file, the page numbering resets after page 400, marking the start of Part 2. To ensure a continuous page count throughout the book, I implemented an offset that allows the numbering to continue past 400 instead of restarting."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('hvd-hw39sc-1696432701.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "lines = text.split('\\n')\n",
    "\n",
    "page_numbers = []\n",
    "page_content = []\n",
    "\n",
    "current_page_number = None\n",
    "current_page_content = []\n",
    "offset = 0  # Initialize an offset\n",
    "\n",
    "for line in lines:\n",
    "    if line.startswith(\"## p. \"):\n",
    "        if current_page_number is not None:\n",
    "            page_numbers.append(current_page_number)\n",
    "            page_content.append(\" \".join(current_page_content))\n",
    "\n",
    "        page_pattern = r'## p\\. (\\d+)'\n",
    "        match = re.match(page_pattern, line)\n",
    "        if match:\n",
    "            # Check for page number reset\n",
    "            if int(match.group(1)) == 1 and current_page_number is not None:\n",
    "                offset += current_page_number  # Update the offset with the last page number\n",
    "\n",
    "            current_page_number = int(match.group(1)) + offset\n",
    "            current_page_content = []\n",
    "    else:\n",
    "        current_page_content.append(line)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As with the fiction example, I created a DataFrame for the full text named `darwin_full_text`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "darwin_full_text = pd.DataFrame({\n",
    "    'page_number': [int(x) for x in page_numbers],\n",
    "    'page_content': page_content,\n",
    "})\n",
    "darwin_full_text[10:15]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### VADER"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "darwin_full_text['vader_sentiment_score'] = 0.0\n",
    "darwin_full_text['vader_sentiment'] = \"\"\n",
    "\n",
    "for tuple in darwin_full_text.itertuples():\n",
    "    sentence = tuple.page_content\n",
    "\n",
    "    sentiment_dictionary = analyzer.polarity_scores(sentence)\n",
    "    compound = sentiment_dictionary['compound']\n",
    "\n",
    "    darwin_full_text.at[tuple.Index, 'vader_sentiment_score'] = compound\n",
    "\n",
    "    if compound >= 0.33:\n",
    "        vader_sentiment = \"Positive\"\n",
    "    elif compound <= -0.33:\n",
    "        vader_sentiment = \"Negative\"\n",
    "    else:\n",
    "        vader_sentiment = \"Neutral\"\n",
    "\n",
    "    darwin_full_text.at[tuple.Index, 'vader_sentiment'] = vader_sentiment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### TextBlob"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "darwin_full_text['textblob_sentiment_score'] = 0.0\n",
    "darwin_full_text['textblob_sentiment'] = \"\"\n",
    "\n",
    "for tuple in darwin_full_text.itertuples():\n",
    "    sentence = tuple.page_content\n",
    "\n",
    "    classifier = TextBlob(sentence)\n",
    "    polarity = classifier.sentiment.polarity\n",
    "\n",
    "    darwin_full_text.at[tuple.Index, 'textblob_sentiment_score'] = polarity\n",
    "\n",
    "    if polarity >= 0.1:\n",
    "        textblob_sentiment = \"Positive\"\n",
    "    elif polarity <= -0.1:\n",
    "        textblob_sentiment = \"Negative\"\n",
    "    else:\n",
    "        textblob_sentiment = \"Neutral\"\n",
    "\n",
    "    darwin_full_text.at[tuple.Index, 'textblob_sentiment'] = textblob_sentiment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### AFINN\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "afinn = Afinn(language='en')\n",
    "\n",
    "darwin_full_text['afinn_sentiment_score'] = 0.0\n",
    "\n",
    "for tuple in darwin_full_text.itertuples():\n",
    "    sentence = tuple.page_content\n",
    "\n",
    "    score = afinn.score(sentence)\n",
    "\n",
    "    darwin_full_text.at[tuple.Index, 'afinn_sentiment_score'] = score\n",
    "\n",
    "# Normalize AFINN scores\n",
    "min_value = min(darwin_full_text['afinn_sentiment_score'])\n",
    "max_value = max(darwin_full_text['afinn_sentiment_score'])\n",
    "\n",
    "normalized_numbers = [(x - min_value) / (max_value - min_value) for x in darwin_full_text['afinn_sentiment_score']]\n",
    "\n",
    "afinn_normalized = [2 * x - 1 for x in normalized_numbers]\n",
    "\n",
    "darwin_full_text['afinn_normalized'] = afinn_normalized\n",
    "\n",
    "darwin_full_text[10:15]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Extracted Features Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "paths = ['hvd.hw39sc.json.bz2']\n",
    "fr = FeatureReader(paths)\n",
    "vol = next(fr.volumes())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "darwin_ef = vol.tokenlist(pos=False, case=False) \\\n",
    "    .reset_index().drop(['section'], axis=1)\n",
    "darwin_ef.columns = ['Page Number', 'token', 'count']\n",
    "grouped_tokens = darwin_ef.groupby('Page Number')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ef_vader_sentiment_score = []\n",
    "ef_textblob_sentiment_score = []\n",
    "ef_afinn_sentiment_score = []\n",
    "\n",
    "for name, group in grouped_tokens:\n",
    "    page_text = \" \".join([word * count for word, count in zip(group['token'], group['count'])])\n",
    "\n",
    "    # VADER Analysis\n",
    "    sentiment_scores = analyzer.polarity_scores(page_text)\n",
    "    ef_vader_sentiment_score.append(sentiment_scores['compound'])\n",
    "\n",
    "    # TextBlob Analysis\n",
    "    sentiment = TextBlob(page_text).sentiment\n",
    "    ef_textblob_sentiment_score.append(sentiment.polarity)\n",
    "\n",
    "    # AFINN Analysis\n",
    "    sentiment_score = afinn.score(page_text)\n",
    "    ef_afinn_sentiment_score.append(sentiment_score)\n",
    "\n",
    "# Create a DataFrame with all sentiment analysis results\n",
    "darwin_ef = pd.DataFrame({\n",
    "    'page_number': [int(x) for x in grouped_tokens.groups.keys()],\n",
    "    'ef_vader_sentiment_score': ef_vader_sentiment_score,\n",
    "    'ef_textblob_sentiment_score': ef_textblob_sentiment_score,\n",
    "    'ef_afinn_sentiment_score': ef_afinn_sentiment_score\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Normalize AFINN scores for EF\n",
    "min_value = min(darwin_ef['ef_afinn_sentiment_score'])\n",
    "max_value = max(darwin_ef['ef_afinn_sentiment_score'])\n",
    "\n",
    "normalized_numbers = [(x - min_value) / (max_value - min_value) for x in darwin_ef['ef_afinn_sentiment_score']]\n",
    "\n",
    "ef_afinn_normalized = [2 * x - 1 for x in normalized_numbers]\n",
    "\n",
    "darwin_ef['ef_afinn_normalized'] = ef_afinn_normalized\n",
    "\n",
    "darwin_ef[10:15]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Subtract the number of pages in front matter from EF DataFrame\n",
    "FRONT_MATTER_PAGES = 16\n",
    "darwin_ef['page_number'] = darwin_ef['page_number'].astype(int) - FRONT_MATTER_PAGES"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Emotional Valence Graph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Smoothing the graph with rolling mean\n",
    "WINDOW_SIZE = 20\n",
    "\n",
    "columns_full_text = ['vader_sentiment_score', 'textblob_sentiment_score', 'afinn_normalized']\n",
    "\n",
    "for col in columns_full_text:\n",
    "    darwin_full_text[col] = darwin_full_text[col].rolling(window=WINDOW_SIZE, min_periods=1).mean()\n",
    "\n",
    "columns_ef = ['ef_vader_sentiment_score', 'ef_textblob_sentiment_score', 'ef_afinn_normalized']\n",
    "\n",
    "for col in columns_ef:\n",
    "    darwin_ef[col] = darwin_ef[col].rolling(window=WINDOW_SIZE, min_periods=1).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "# Plotting for dumas_full_text DataFrame\n",
    "sentiment_scores_full_text = {\n",
    "    'vader_sentiment_score': 'Full Text VADER',\n",
    "    'textblob_sentiment_score': 'Full Text TextBlob',\n",
    "    'afinn_normalized': 'Full Text AFINN Normalized'\n",
    "}\n",
    "\n",
    "for column, label in sentiment_scores_full_text.items():\n",
    "    fig.add_trace(go.Scatter(x=darwin_full_text['page_number'], y=darwin_full_text[column], mode='lines', name=label))\n",
    "\n",
    "# Plotting for dumas_ef DataFrame\n",
    "sentiment_scores_ef = {\n",
    "    'ef_vader_sentiment_score': 'EF VADER',\n",
    "    'ef_textblob_sentiment_score': 'EF TextBlob',\n",
    "    'ef_afinn_normalized': 'EF AFINN Normalized'\n",
    "}\n",
    "\n",
    "for column, label in sentiment_scores_ef.items():\n",
    "    fig.add_trace(go.Scatter(x=darwin_ef['page_number'], y=darwin_ef[column], mode='lines', name=label))\n",
    "\n",
    "# Additional plot settings\n",
    "fig.update_layout(\n",
    "    title=f'Emotional Valence throughout \"The Origin of Species\"',\n",
    "    xaxis_title='Page Number',\n",
    "    yaxis_title='Sentiment Score',\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exploring Large Language Models (LLMs)\n",
    "During my project, I experimented with sentiment analysis using Large Language Models (LLMs) such as BERTweet and SiEBERT. However, the approach encountered several challenges. For the full text analysis, the length of content on each page often exceeded the token limit of these models. I attempted to segment the page content into individual sentences using spaCy, but occasionally, even these sentences were too lengthy. Truncating sentences to fit within the token limit compromised the accuracy of the analysis.\n",
    "\n",
    "In the case of Extracted Features, the use of LLMs proved impractical. The token lists comprised isolated words without context, which contradicts the advantage of LLMs: analyzing more extended sentences or texts to understand the overall sentiment. Additionally, the computational demands of running LLMs exceeded the capabilities of my available hardware.\n",
    "\n",
    "Given these limitations and the challenges, I ultimately decided against including LLMs in the final iteration of my project.\n",
    "\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "In analyzing the graphs for both the fiction and nonfiction examples, several key observations emerged:\n",
    "- The disparity in sentiment scores between the full text and Extracted Features using the same tool was relatively minor. In contrast, there were more significant variations when comparing different tools analyzing the same input.\n",
    "- TextBlob's sentiment scores generally hovered closer to neutral (0), while VADER's scores showed a greater deviation from neutrality.\n",
    "- As for the overall sentiment direction, VADER typically presented more positive scores. TextBlob's scores were moderately positive, while AFINN's scores leaned more towards the negative spectrum.\n",
    "\n",
    "These findings highlight the distinct characteristics and tendencies of each sentiment analysis tool when applied to both fiction and nonfiction works.\n",
    "\n",
    "In closing, I would like to express my gratitude to Glen Layne-Worthey for his invaluable guidance and encouragement throughout this project. I am also deeply thankful to Ryan Dubnicek for sharing his technical expertise, which greatly aided this work."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Further Readings\n",
    "Bowers, Katherine and Quinn Dombrowski. “Katia and the Sentiment Snobs”. The Data-Sitters Club. October 25, 2021. https://datasittersclub.github.io/site/dsc11.html.\n",
    "\n",
    "Organisciak, Peter and Boris Capitanu. \"Text Mining in Python through the HTRC Feature Reader.\" Programming Historian 5 (2016). https://doi.org/10.46430/phen0058."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
